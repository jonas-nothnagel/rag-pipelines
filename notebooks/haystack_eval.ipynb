{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import json\n",
    "from typing import Tuple, List\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack.components.builders import PromptBuilder, AnswerBuilder\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "from haystack.components.evaluators import ContextRelevanceEvaluator, FaithfulnessEvaluator, SASEvaluator\n",
    "from haystack.evaluation import EvaluationRunResult\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from openai import BadRequestError\n",
    "from getpass import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
    "generator = OpenAIGenerator(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set base path\n",
    "base_path = \"../data/evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "import json\n",
    "from typing import Tuple, List\n",
    "\n",
    "def read_question_answers() -> Tuple[List[str], List[str]]:\n",
    "    with open(base_path+\"/eval_data.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        questions = data[\"questions\"]\n",
    "        answers = data[\"ground_truths\"]\n",
    "    return questions, answers\n",
    "\n",
    "questions, answers = read_question_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document loading, indexing and embedding\n",
    "def indexing(embedding_model: str, chunk_size: int):\n",
    "    files_path = base_path+\"/eval_documents\"\n",
    "    document_store = InMemoryDocumentStore()\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_component(\"converter\", PyPDFToDocument())\n",
    "    pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
    "    pipeline.add_component(\"splitter\", DocumentSplitter(split_length=chunk_size))  # splitting by word\n",
    "    pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store, policy=DuplicatePolicy.SKIP))\n",
    "    pipeline.add_component(\"embedder\", SentenceTransformersDocumentEmbedder(embedding_model))\n",
    "    pipeline.connect(\"converter\", \"cleaner\")\n",
    "    pipeline.connect(\"cleaner\", \"splitter\")\n",
    "    pipeline.connect(\"splitter\", \"embedder\")\n",
    "    pipeline.connect(\"embedder\", \"writer\")\n",
    "    pdf_files = [files_path+\"/\"+f_name for f_name in os.listdir(files_path)]\n",
    "    pipeline.run({\"converter\": {\"sources\": pdf_files}})\n",
    "\n",
    "    return document_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define basic RAG pipeline\n",
    "def basic_rag(document_store, embedding_model, top_k=2):\n",
    "    template = \"\"\"\n",
    "        You have to answer the following question based on the given context information only.\n",
    "        If the context is empty or just a '\\n' answer with None, example: \"None\".\n",
    "\n",
    "        Context:\n",
    "        {% for document in documents %}\n",
    "            {{ document.content }}\n",
    "        {% endfor %}\n",
    "\n",
    "        Question: {{question}}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    basic_rag = Pipeline()\n",
    "    basic_rag.add_component(\"query_embedder\", SentenceTransformersTextEmbedder(\n",
    "        model=embedding_model, progress_bar=False\n",
    "    ))\n",
    "    basic_rag.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store, top_k=top_k))\n",
    "    basic_rag.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
    "    basic_rag.add_component(\"llm\", OpenAIGenerator(model=\"gpt-3.5-turbo\"))\n",
    "    basic_rag.add_component(\"answer_builder\", AnswerBuilder())\n",
    "\n",
    "    basic_rag.connect(\"query_embedder\", \"retriever.query_embedding\")\n",
    "    basic_rag.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "    basic_rag.connect(\"prompt_builder\", \"llm\")\n",
    "    basic_rag.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "    basic_rag.connect(\"llm.meta\", \"answer_builder.meta\")\n",
    "    basic_rag.connect(\"retriever\", \"answer_builder.documents\")\n",
    "\n",
    "    return basic_rag\n",
    "\n",
    "def run_basic_rag(doc_store, sample_questions, embedding_model, top_k):\n",
    "    \"\"\"\n",
    "    A function to run the basic rag model on a set of sample questions and answers\n",
    "    \"\"\"\n",
    "\n",
    "    rag = basic_rag(document_store=doc_store, embedding_model=embedding_model, top_k=top_k)\n",
    "\n",
    "    predicted_answers = []\n",
    "    retrieved_contexts = []\n",
    "    for q in tqdm(sample_questions):\n",
    "        try:\n",
    "            response = rag.run(\n",
    "                data={\"query_embedder\": {\"text\": q}, \"prompt_builder\": {\"question\": q}, \"answer_builder\": {\"query\": q}})\n",
    "            predicted_answers.append(response[\"answer_builder\"][\"answers\"][0].data)\n",
    "            retrieved_contexts.append([d.content for d in response['answer_builder']['answers'][0].documents])\n",
    "        except BadRequestError as e:\n",
    "            print(f\"Error with question: {q}\")\n",
    "            print(e)\n",
    "            predicted_answers.append(\"error\")\n",
    "            retrieved_contexts.append(retrieved_contexts)\n",
    "\n",
    "    return retrieved_contexts, predicted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(sample_questions, sample_answers, retrieved_contexts, predicted_answers, embedding_model):\n",
    "    context_relevance = ContextRelevanceEvaluator(raise_on_failure=False)\n",
    "    faithfulness = FaithfulnessEvaluator(raise_on_failure=False)\n",
    "    sas = SASEvaluator(model=embedding_model)\n",
    "    sas.warm_up()\n",
    "\n",
    "    results = {\n",
    "        \"context_relevance\": context_relevance.run(sample_questions, retrieved_contexts),\n",
    "        \"faithfulness\": faithfulness.run(sample_questions, retrieved_contexts, predicted_answers),\n",
    "        \"sas\": sas.run(predicted_answers, sample_answers),\n",
    "    }\n",
    "\n",
    "    inputs = {'questions': sample_questions, \"true_answers\": sample_answers, \"predicted_answers\": predicted_answers}\n",
    "\n",
    "    return results, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_tuning(questions, answers, out_path: str):\n",
    "    \"\"\"\n",
    "    Run the basic RAG model with different parameters, and evaluate the results.\n",
    "\n",
    "    The parameters to be tuned are: embedding model, top_k, and chunk_size.\n",
    "    \"\"\"\n",
    "    embedding_models = {\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"sentence-transformers/msmarco-distilroberta-base-v2\",\n",
    "        \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    }\n",
    "    top_k_values = [1, 2, 3]\n",
    "    chunk_sizes = [64, 128, 256]\n",
    "\n",
    "    # create results directory if it does not exist using Pathlib\n",
    "    out_path = Path(out_path)\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "\n",
    "    for embedding_model in embedding_models:\n",
    "        for chunk_size in chunk_sizes:\n",
    "            print(f\"Indexing documents with {embedding_model} model with a chunk_size={chunk_size}\")\n",
    "            doc_store = indexing(embedding_model, chunk_size)\n",
    "            for top_k in top_k_values:\n",
    "                name_params = f\"{embedding_model.split('/')[-1]}__top_k:{top_k}__chunk_size:{chunk_size}\"\n",
    "                print(name_params)\n",
    "                print(\"Running RAG pipeline\")\n",
    "                retrieved_contexts, predicted_answers = run_basic_rag(doc_store, questions, embedding_model, top_k)\n",
    "                print(f\"Running evaluation\")\n",
    "                results, inputs = run_evaluation(questions, answers, retrieved_contexts, predicted_answers, embedding_model)\n",
    "                eval_results = EvaluationRunResult(run_name=name_params, inputs=inputs, results=results)\n",
    "                eval_results.score_report().to_csv(f\"{out_path}/score_report_{name_params}.csv\",index=False)\n",
    "                eval_results.to_pandas().to_csv(f\"{out_path}/detailed_{name_params}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_tuning(questions, answers, \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_results(f_name: str):\n",
    "    pattern = r\"score_report_(.*?)__top_k:(\\\\d+)__chunk_size:(\\\\d+)\\\\.csv\"\n",
    "    match = re.search(pattern, f_name)\n",
    "    if match:\n",
    "        embeddings_model = match.group(1)\n",
    "        top_k = int(match.group(2))\n",
    "        chunk_size = int(match.group(3))\n",
    "        return embeddings_model, top_k, chunk_size\n",
    "    else:\n",
    "        print(\"No match found\")\n",
    "\n",
    "def read_scores(path: str):\n",
    "    all_scores = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f_name in files:\n",
    "            if not f_name.startswith(\"score_report\"):\n",
    "                continue\n",
    "\n",
    "            embeddings_model, top_k, chunk_size = parse_results(f_name)\n",
    "\n",
    "            df = pd.read_csv(path+\"/\"+f_name)\n",
    "\n",
    "            df.rename(columns={'Unnamed: 0': 'metric'}, inplace=True)\n",
    "            df_transposed = df.T\n",
    "            df_transposed.columns = df_transposed.iloc[0]\n",
    "            df_transposed = df_transposed[1:]\n",
    "\n",
    "            # Add new columns\n",
    "            df_transposed['embeddings'] = embeddings_model\n",
    "            df_transposed['top_k'] = top_k\n",
    "            df_transposed['chunk_size'] = chunk_size\n",
    "\n",
    "            all_scores.append(df_transposed)\n",
    "\n",
    "    df = pd.concat(all_scores)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.rename_axis(None, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results/score_report_msmarco-distilroberta-base-v2__top_k:2__chunk_size:64.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match found\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mread_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 23\u001b[0m, in \u001b[0;36mread_scores\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_report\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m embeddings_model, top_k, chunk_size \u001b[38;5;241m=\u001b[39m parse_results(f_name)\n\u001b[1;32m     25\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mf_name)\n\u001b[1;32m     27\u001b[0m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "df = read_scores('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-uganda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
